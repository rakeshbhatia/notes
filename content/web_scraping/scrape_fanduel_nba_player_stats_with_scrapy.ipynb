{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Fanduel NBA Player Stats with Scrapy\n",
    "06 April 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fanduel player stats archive: \n",
    "## http://rotoguru1.com/cgi-bin/hyday.pl?mon=4&day=10&year=2019&game=fd\n",
    "\n",
    "## Overview\n",
    "* Create a simple scraper to obtain NBA player daily fantasy basketball stats from the RotoGuru site, using Scrapy\n",
    "* We will focus on extracting fantasy stats for Fanduel only, but the same methodology can be used to extract stats for other platforms (i.e., Draftkings)\n",
    "* The RotoGuru site uses pagination, storing data from one day's worth of games on a single page\n",
    "* We will enable our crawler to identify the next page link (for the previous day's games) and recursively crawl these links until we reach the first day of the season\n",
    "    * This will enable us to obtain an entire season's worth of data\n",
    "* For the purposes of this demonstration, we will set a depth limit to prevent our scraper from crawling all the links in succession, which may overload the site's server\n",
    "* Before defining our scraper, we must visually inspect the site's source code and identify where our target data is located\n",
    "* Scrapy allows us to use CSS or XPath selectors; for this project we will use XPATH\n",
    "* Navigating to the website and inspecting the source reveals that our required data is located in a table with the following XPath:\n",
    "    * /html/body/table[1]//table[@cellspacing=5]\n",
    "* Each row of data in this table contains the stats for a single player from that day's game\n",
    "* Stats include standard NBA box score numbers, as well as total Fanduel points scored\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin creating our scraper\n",
    "* First declare the spider class\n",
    "* Create a name for the spider\n",
    "* Provide a list of start urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new spider class\n",
    "class DFSpider(scrapy.Spider):\n",
    "    # Name our spider\n",
    "    name = \"DFS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://rotoguru1.com/cgi-bin/hyday.pl?mon=4&day=10&year=2019&game=fd'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the `parse` method\n",
    "* Get all the table rows and store them in a list\n",
    "* Remove the first two rows as they don't contain any player data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new spider class\n",
    "class DFSpider(scrapy.Spider):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Use XPath to parse the response we get\n",
    "    def parse(self, response):\n",
    "        # Get the table rows\n",
    "        trs = response.xpath('/html/body/table[1]//table[@cellspacing=5]//tr')\n",
    "            \n",
    "        # Remove first two rows since they don't contain any player data\n",
    "        trs = trs[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate across table rows and extract player stats\n",
    "* Yield a dictionary containing our extracted data\n",
    "* Use the XPath for the `td` elements located in each `tr` element of the data\n",
    "* Extract only the text content from each `td` element (or from the appropriate enclosed element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new spider class\n",
    "class DFSpider(scrapy.Spider):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Parse the response with XPath\n",
    "    def parse(self, response):\n",
    "\n",
    "        # ...\n",
    "        \n",
    "        # Check if table rows exist\n",
    "        if trs:        \n",
    "            # Iterate over each row\n",
    "            for tr in trs:\n",
    "                # Yield a dictionary with our desired values\n",
    "                yield {\n",
    "                    # Extract each player's stats here\n",
    "                    'Name': tr.xpath('./td[2]/a/text()').extract(),\n",
    "                    'Position': tr.xpath('./td[1]/text()').extract(),\n",
    "                    'FD Pts': tr.xpath('./td[3]/text()').extract(),\n",
    "                    'FD Salary': tr.xpath('./td[4]/text()').extract(),\n",
    "                    'Team': tr.xpath('./td[5]/text()').extract(),\n",
    "                    'Opp': tr.xpath('./td[6]/text()').extract(),\n",
    "                    'Score': tr.xpath('./td[7]/text()').extract(),\n",
    "                    'Min': tr.xpath('./td[8]/text()').extract(),\n",
    "                    'Stats': tr.xpath('./td[9]/text()').extract()\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for the next page link\n",
    "* We will call our parse function on the next page link\n",
    "* In this manner, we will recursively crawl across the series of links until we either hit our depth (in settings), or reach the beginning of the archive (link to opening day's stats page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new spider class\n",
    "class DFSpider(scrapy.Spider):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Parse the response with XPath\n",
    "    def parse(self, response):\n",
    "\n",
    "        # ...\n",
    "        \n",
    "        # ...\n",
    "\n",
    "        # Select the table containing the next page link\n",
    "        table = response.xpath('//table[@border=0]')[6]\n",
    "\n",
    "        # Get the next page link\n",
    "        next_page = table.xpath('./tr[1]/td[1]/a/@href').extract_first()\n",
    "\n",
    "        # Recursively call the parse function on the next page link\n",
    "        if next_page is not None:\n",
    "            print('Page completed. Going to next page.')\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass in desired settings and start the crawler\n",
    "* Save the data in JSON format\n",
    "* Set a download delay of 0.50 seconds to minimize server load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new spider class\n",
    "class DFSpider(scrapy.Spider):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Parse the response with XPath\n",
    "    def parse(self, response):\n",
    "\n",
    "        # ...\n",
    "        \n",
    "        # ...\n",
    "\n",
    "        # ...\n",
    "\n",
    "# Pass in settings\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',                 # Save our data as json\n",
    "    'FEED_URI': 'nba_fanduel_stats.json',  # Specify the json output file\n",
    "    'DEPTH_LIMIT': 3,                      # Only traverse three links\n",
    "    'DOWNLOAD_DELAY': 0.50,                # Set a delay of 0.5 seconds\n",
    "    'LOG_ENABLED': False                   # For debugging, change this to true\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our complete scrapy crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create a new spider class\n",
    "class RotoSpider(scrapy.Spider):\n",
    "    # Name our spider\n",
    "    name = \"RS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'http://rotoguru1.com/cgi-bin/hyday.pl?mon=4&day=10&year=2019&game=fd'\n",
    "    ]\n",
    "    \n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        # Get the table rows\n",
    "        trs = response.xpath('/html/body/table[1]//table[@cellspacing=5]//tr')\n",
    "            \n",
    "        # Remove first two rows since they don't contain any player data\n",
    "        trs = trs[2:]   \n",
    "        \n",
    "        # Iterate over every element on the page\n",
    "        if trs:\n",
    "            for tr in trs:\n",
    "                # Yield a dictionary with the values we want\n",
    "                yield {\n",
    "                    'Name': tr.xpath('./td[2]/a/text()').extract(),\n",
    "                    'Position': tr.xpath('./td[1]/text()').extract(),\n",
    "                    'FD Pts': tr.xpath('./td[3]/text()').extract(),\n",
    "                    'FD Salary': tr.xpath('./td[4]/text()').extract(),\n",
    "                    'Team': tr.xpath('./td[5]/text()').extract(),\n",
    "                    'Opp': tr.xpath('./td[6]/text()').extract(),\n",
    "                    'Score': tr.xpath('./td[7]/text()').extract(),\n",
    "                    'Min': tr.xpath('./td[8]/text()').extract(),\n",
    "                    'Stats': tr.xpath('./td[9]/text()').extract()\n",
    "                }\n",
    "                \n",
    "        # Select the table containing the next page link\n",
    "        table = response.xpath('//table[@border=0]')[6]\n",
    "        \n",
    "        # Get the next page link\n",
    "        next_page = table.xpath('./tr[1]/td[1]/a/@href').extract_first()\n",
    "        \n",
    "        # Run the parse function recursively on the next page link\n",
    "        if next_page is not None:\n",
    "            print('Page completed. Going to next page.')\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "        \n",
    "# Pass in settings\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',                 # Save our data as json\n",
    "    'FEED_URI': 'nba_fanduel_stats.json',  # Specify the json output file\n",
    "    'DEPTH_LIMIT': 3,                      # Only traverse three links\n",
    "    'DOWNLOAD_DELAY': 0.50,                # Set a delay of 0.5 seconds\n",
    "    'LOG_ENABLED': False                   # For debugging, change this to true\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page completed. Going to next page.\n",
      "Page completed. Going to next page.\n",
      "Page completed. Going to next page.\n",
      "Page completed. Going to next page.\n",
      "Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "# Start the crawler\n",
    "process.crawl(RotoSpider)\n",
    "process.start()\n",
    "print('Scraping completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import JSON data into new pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1189\n",
      "   FD Pts  FD Salary      Min                  Name      Opp Position  \\\n",
      "0  [58.7]   [$3,500]  [48:00]    [Simons, Anfernee]  [v sac]     [SG]   \n",
      "1  [57.4]   [$3,500]  [40:43]      [Allen, Grayson]  [@ lac]     [SG]   \n",
      "2  [56.9]   [$9,800]  [40:09]       [Walker, Kemba]  [v orl]     [PG]   \n",
      "3  [55.7]   [$3,600]  [48:00]        [Frazier, Tim]  [v okc]     [PG]   \n",
      "4  [55.7]  [$12,100]  [36:28]  [Westbrook, Russell]  [@ mil]     [PG]   \n",
      "\n",
      "        Score                                              Stats   Team  \n",
      "0  [ 136-131]      [   37pt 6rb 9as 1st 2to 7trey 13-21fg 4-6ft]  [por]  \n",
      "1  [ 137-143]  [   40pt 7rb 4as 1st 1bl 3to 5trey 11-30fg 13-...  [uta]  \n",
      "2  [ 114-122]      [   43pt 2rb 5as 2bl 2to 4trey 16-25fg 7-7ft]  [cha]  \n",
      "3  [ 116-127]     [   29pt 6rb 13as 1bl 3to 4trey 10-23fg 5-7ft]  [mil]  \n",
      "4  [ 127-116]  [   15pt 11rb 17as 1st 1bl 4to 1trey 7-10fg 0-...  [okc]  \n"
     ]
    }
   ],
   "source": [
    "nba_fanduel_stats = pd.read_json('nba_fanduel_stats.json', orient='records')\n",
    "print('Number of rows: {}'.format(nba_fanduel_stats.shape[0]))\n",
    "print(nba_fanduel_stats.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate the basics of Scrapy by scraping the homepage of this website. Note: the site has been updated since this notebook was written, so scraped content shown here will not reflect any new changes. It doesn't matter as this notebook is simply for demonstration purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![notes_website_home.png](../../assets/img/notes_website_home.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect site source code: https://rakeshbhatia.github.io/notes/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![notes_website_source.png](../../assets/img/notes_website_source.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin creating our scraper\n",
    "* Declare your spider class\n",
    "* Create a name for the spider\n",
    "* Provide a list of start urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new spider class\n",
    "class NotesSpider(scrapy.Spider):\n",
    "    # Name our spider\n",
    "    name = \"NS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://rakeshbhatia.github.io/notes/'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy crawler using XPath selectors\n",
    "* Selectors are a mechanism to extract data from an XML document\n",
    "* XPath is a language used for selecting nodes in XML documents\n",
    "* XPath selectors allow you to navigate anywhere inside the DOM tree\n",
    "    * They are more powerful than CSS selectors, which have limited navigation\n",
    "    * They enable selection, filtering, and fine-grained text extraction\n",
    "* The `xpath()` method returns a `SelectorList` instance, which is a list of new selectors\n",
    "* Nested data can be quickly selected from the list returned by `xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the `parse` method\n",
    "* Extract the main header `h2` tag element\n",
    "* Extract the subheader `h2` tag element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotesSpider(scrapy.Spider):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Use XPath to parse the response we get\n",
    "    def parse(self, response):\n",
    "        main_header = response.xpath('//*[@id=\"python--data-science--machine-learning\"]').extract()  \n",
    "        print('main_header: ', main_header)\n",
    "\n",
    "        sub_header = response.xpath('//*[@id=\"technical-notes\"]').extract()\n",
    "        print('sub_header: ', sub_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract `li` tag list elements from the first `blockquote` tag element\n",
    "* First use `xpath()` to obtain a list of selectors containing data for the `li` tag elements\n",
    "* Then iterate through the list of selectors to extract the following:\n",
    "    * Text from the nested `a` tag element\n",
    "    * Text from the `href` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotesSpider(scrapy.Spider):\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # Use XPath to parse the response we get\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        # Get li tag elements from first blockquote\n",
    "        list_items = response.xpath('/html/body/section[2]/blockquote[1]/ul//li')\n",
    "        print('list_items: ', list_items)            \n",
    "        \n",
    "        # Yield each post title and post url to output json file\n",
    "        for li in list_items:\n",
    "            yield {\n",
    "                'Post Title': li.xpath('./a/text()').extract_first(),\n",
    "                'Post URL': li.xpath('./a/@href').extract_first()\n",
    "            }\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass in settings and start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotesSpider(scrapy.Spider):\n",
    "    \n",
    "    # ...\n",
    "\n",
    "    # Use XPath to parse the response we get\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "        # ...\n",
    "        \n",
    "# Pass in settings\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',           # Save our data as json\n",
    "    'FEED_URI': 'scrapy_basics.json', # Specify the json output file\n",
    "    'DOWNLOAD_DELAY': 0.50,          # Set a delay of 0.5 seconds\n",
    "    'LOG_ENABLED': False             # For debugging, change this to true\n",
    "})\n",
    "\n",
    "# Start the crawler\n",
    "process.crawl(NotesSpider)\n",
    "process.start()\n",
    "print('Scraping completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our complete scrapy crawler using XPath selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_header:  Python • Data Science • Machine Learning\n",
      "second_header:  Technical Notes\n",
      "list_items:  [<Selector xpath='/html/body/section[2]/blockquote[1]/ul//li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='/html/body/section[2]/blockquote[1]/ul//li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='/html/body/section[2]/blockquote[1]/ul//li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='/html/body/section[2]/blockquote[1]/ul//li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='/html/body/section[2]/blockquote[1]/ul//li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='/html/body/section[2]/blockquote[1]/ul//li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='/html/body/section[2]/blockquote[1]/ul//li' data='<li><a href=\"https://rakeshbhatia.github'>]\n",
      "Post Title:  Sets\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/sets\n",
      "Post Title:  Loops\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/loops\n",
      "Post Title:  If Else\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/if_else\n",
      "Post Title:  Dictionaries\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/dictionaries\n",
      "Post Title:  Linked Lists\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/linked_lists\n",
      "Post Title:  Pandas Basics\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/pandas_basics\n",
      "Post Title:  List Comprehension\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/list_comprehension\n",
      "Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create a new spider class\n",
    "class NotesSpider(scrapy.Spider):\n",
    "    # Name our spider\n",
    "    name = \"NS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://rakeshbhatia.github.io/notes/'\n",
    "    ]\n",
    "\n",
    "    # Parse the response using XPath selectors\n",
    "    def parse(self, response):\n",
    "        # Get first h2 text\n",
    "        first_header = response.xpath('//*[@id=\"python--data-science--machine-learning\"]/center/text()').extract_first()\n",
    "        print('first_header: ', first_header)\n",
    "        \n",
    "        #Get second h2 text\n",
    "        second_header = response.xpath('//*[@id=\"technical-notes\"]/center/text()').extract_first()\n",
    "        print('second_header: ', second_header)\n",
    "        \n",
    "        # Get a list of selectors with li tag elements from the first blockquote\n",
    "        list_items = response.xpath('/html/body/section[2]/blockquote[1]/ul//li')\n",
    "        print('list_items: ', list_items)            \n",
    "        \n",
    "        # Yield each post title and post url to output json file\n",
    "        for li in list_items:\n",
    "            print('Post Title: ', li.xpath('./a/text()').extract_first())\n",
    "            print('Post URL: ', li.xpath('./a/@href').extract_first())\n",
    "            yield {\n",
    "                'Post Title': li.xpath('./a/text()').extract_first(),\n",
    "                'Post URL': li.xpath('./a/@href').extract_first()\n",
    "            }\n",
    "            \n",
    "# Pass in settings\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',           # Save our data as json\n",
    "    'FEED_URI': 'scrapy_basics_xpath.json', # Specify the json output file\n",
    "    'DOWNLOAD_DELAY': 0.50,          # Set a delay of 0.5 seconds\n",
    "    'LOG_ENABLED': False             # For debugging, change this to true\n",
    "})\n",
    "\n",
    "# Start the crawler\n",
    "process.crawl(NotesSpider)\n",
    "process.start()\n",
    "print('Scraping completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect JSON output file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scrapy_basics_xpath_json.png](../../assets/img/scrapy_basics_xpath_json.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use JSON as our feed format, we must yield our desired data in the form of a dictionary-like object. Sure enough, we can see that our JSON data was correctly printed to the output file. We have our requisite JSON dictionary object consisting of two keys, \"Post Title\" and \"Post URL\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy crawler using CSS selectors\n",
    "* CSS is a styling language for HTML documents\n",
    "* CSS selectors can select attribute or text nodes using CSS3 pseudo-elements\n",
    "* Every step will be virtually the same as our scraper using XPath selectors\n",
    "    * The only difference will be the use of the `.css()` function instead of `.xpath()`\n",
    "* For most cases, you will likely use XPath selectors, which are more flexible and robust\n",
    "    * However, in some specific instances, it might be advantageous to use CSS selectors instead\n",
    "    * For example, if you want to locate a `button` element that has a particular `id` and/or `class` attribute\n",
    "* Note the differences in the form of the argument to the `xpath()` and `css()` functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our complete scrapy crawler using CSS selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_header:  Python • Data Science • Machine Learning\n",
      "second_header:  Technical Notes\n",
      "list_items:  [<Selector xpath='descendant-or-self::li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='descendant-or-self::li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='descendant-or-self::li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='descendant-or-self::li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='descendant-or-self::li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='descendant-or-self::li' data='<li><a href=\"https://rakeshbhatia.github'>, <Selector xpath='descendant-or-self::li' data='<li><a href=\"https://rakeshbhatia.github'>]\n",
      "Post Title:  Sets\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/sets\n",
      "Post Title:  Loops\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/loops\n",
      "Post Title:  If Else\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/if_else\n",
      "Post Title:  Dictionaries\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/dictionaries\n",
      "Post Title:  Linked Lists\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/linked_lists\n",
      "Post Title:  Pandas Basics\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/pandas_basics\n",
      "Post Title:  List Comprehension\n",
      "Post URL:  https://rakeshbhatia.github.io/notes/content/python/list_comprehension\n",
      "Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create a new spider class\n",
    "class NotesSpider(scrapy.Spider):\n",
    "    # Name our spider\n",
    "    name = \"NS\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://rakeshbhatia.github.io/notes/'\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get\n",
    "    def parse(self, response):\n",
    "        # Get first h2 text\n",
    "        first_header = response.css('h2[id=\"python--data-science--machine-learning\"] center::text').extract_first()\n",
    "        print('first_header: ', first_header)\n",
    "        \n",
    "        # Get second h2 text\n",
    "        second_header = response.css('h2[id=\"technical-notes\"] center::text').extract_first()\n",
    "        print('second_header: ', second_header)\n",
    "        \n",
    "        # Get a list of selectors with li tag elements from the first blockquote\n",
    "        list_items = response.css('blockquote ul')[0].css('li')        \n",
    "        print('list_items: ', list_items)            \n",
    "        \n",
    "        # Yield each post title and post url to output json file\n",
    "        for li in list_items:\n",
    "            print('Post Title: ', li.css('a::text').extract_first())\n",
    "            print('Post URL: ', li.css('a::attr(href)').extract_first())\n",
    "            yield {\n",
    "                'Post Title': li.css('a::text').extract_first(),\n",
    "                'Post URL': li.css('a::attr(href)').extract_first()\n",
    "            }\n",
    "        \n",
    "# Pass in settings\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',           # Save our data as json\n",
    "    'FEED_URI': 'scrapy_basics_css.json', # Specify the json output file\n",
    "    'DOWNLOAD_DELAY': 0.50,          # Set a delay of 0.5 seconds\n",
    "    'LOG_ENABLED': False             # For debugging, change this to true\n",
    "})\n",
    "\n",
    "# Start the crawler\n",
    "process.crawl(NotesSpider)\n",
    "process.start()\n",
    "print('Scraping completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect JSON output file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scrapy_basics_css_json.png](../../assets/img/scrapy_basics_css_json.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our output file is the same as before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

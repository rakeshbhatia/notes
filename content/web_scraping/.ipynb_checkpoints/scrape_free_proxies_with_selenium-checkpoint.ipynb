{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6bdb56",
   "metadata": {},
   "source": [
    "# **Scrape Free Proxies with Selenium**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a083446",
   "metadata": {},
   "source": [
    "In this tutorial, you'll learn how to scrape a list of free proxies that you can use for web scraping. One of the most reliable sources of free proxies is at [spys.one](https://spys.one/en/anonymous-proxy-list/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a9f53",
   "metadata": {},
   "source": [
    "![spys-proxy-list-1.png](../../assets/img/spys-proxy-list-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e40e5e",
   "metadata": {},
   "source": [
    "The proxy information is partially rendered in JavaScript, so it's best to utilize a Selenium headless browser to scrape this data. First, we'll import the required libraries for Selenium, as well as time, Pandas, Fake UserAgent, and Undetected ChromeDriver. Time is already included with Python, but case you don't have any of the other libraries, you can install them using the following commands:\n",
    "\n",
    "`pip install -U selenium`\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "`pip install fake-useragent`\n",
    "\n",
    "`pip install undetected-chromedriver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66999a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e22ff9",
   "metadata": {},
   "source": [
    "Let's create a function to instantiate a WebDriver instance. We will use Undetected ChromeDriver for this instance, which you can get from [here](https://github.com/ultrafunkamsterdam/undetected-chromedriver). Undetected ChromeDriver is a special ChromeDriver that helps to evade detection while scraping dynamic websites. The basic usage is essentially the same as regular ChromeDriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4386f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser():\n",
    "    # Get random user agent\n",
    "    ua = UserAgent()\n",
    "    user_agent = ua.random\n",
    "    # Set options for undetected chromedriver\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    # Instantiate driver with options\n",
    "    driver = uc.Chrome(options=options, use_subprocess=True)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae29972",
   "metadata": {},
   "source": [
    "Next, we will define our main scraper function. We'll start by instantiating our WebDriver instance using the method we created above. Then, we will use Selenium's `get()` method to retrieve the URL with our WebDriver instance. Since the page takes a few seconds to load, we'll make the program sleep for 3 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proxies():\n",
    "    url = \"https://spys.one/en/anonymous-proxy-list/\"\n",
    "\n",
    "    # Instantiate the driver\n",
    "    driver = browser()\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c60ba7",
   "metadata": {},
   "source": [
    "There are two dropdown menus that we need to select an option from: the first is \"Show\" and the second is \"Type.\" We want to show the maximum possible number of proxies, which is 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f93b525",
   "metadata": {},
   "source": [
    "![spys-proxy-list-2.png](../../assets/img/spys-proxy-list-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969aa8f",
   "metadata": {},
   "source": [
    "Let's inspect the element to see exactly what we need to scrape. The first dropdown menu is represented by a `<select>` tag with an `id` attribute of `xpp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f0e40",
   "metadata": {},
   "source": [
    "![spys-proxy-list-3.png](../../assets/img/spys-proxy-list-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6668cd",
   "metadata": {},
   "source": [
    "The type of proxies we want is HTTP, so we'll select that from the other dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa4c94",
   "metadata": {},
   "source": [
    "![spys-proxy-list-4.png](../../assets/img/spys-proxy-list-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907ac3d",
   "metadata": {},
   "source": [
    "Inspecting the other dropdown element, we can see that it is represented by a `<select>` tag with `id` attribute of `xf5`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc141f4f",
   "metadata": {},
   "source": [
    "![spys-proxy-list-5.png](../../assets/img/spys-proxy-list-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e69df",
   "metadata": {},
   "source": [
    "We'll use the `driver.find_element()` function to select the dropdown menus by `id`, and use these to create a `Select` object. Then we can use the `driver.select_by_visible_text()` function to select our desired options. After each selection, we'll insert a `sleep` of 5 seconds to give the page time to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proxies():\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    # Select option from dropdown menu for \"Show\"\n",
    "    select_show = Select(driver.find_element(By.ID, \"xpp\"))\n",
    "    select_show.select_by_visible_text(\"500\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Select option from dropdown menu for \"Type\"\n",
    "    select_type = Select(driver.find_element(By.ID, \"xf5\"))\n",
    "    select_type.select_by_visible_text(\"HTTP\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4615a3c",
   "metadata": {},
   "source": [
    "Now let's inspect the elements that contain the data we need. We only need the first two columns of the table, which display the proxy address and proxy type. So, for each row of the `<table>`, we will only select the first two `td` elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885d245",
   "metadata": {},
   "source": [
    "![spys-proxy-list-6.png](../../assets/img/spys-proxy-list-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b5fd2",
   "metadata": {},
   "source": [
    "We'll use the `driver.find_elements()` function to get all the `tr` elements in the HTML and store them in a list called `rows`. There are extra rows that don't list any proxy information, so we'll remove these from the list. Based on inspecting the rows that are retrieved, the rows we need are every other row from indexes `9` to `len(rows)-4`. We will use Python's slicing feature to filter out the unnecessary rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proxies():\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    # Get rows\n",
    "    rows = driver.find_elements(By.TAG_NAME, \"tr\")\n",
    "    # Filter out unnecessary rows\n",
    "    rows = rows[9:len(rows)-4:2]\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb3e00",
   "metadata": {},
   "source": [
    "Let's create a new list called `ip_addresses`. Now we can loop through all the rows and retrieve the td elements that we need. The proxy address and port are listed in the first column. We also need to check whether the proxy is of type HTTP or HTTPs. On each iteration, we'll instantiate a new dictionary that stores these two pieces of data and append the dictionary to `ip_addresses`. This will allow us to save the proxy information to a Pandas DataFrame later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b18521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proxies():\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    # Loop through rows\n",
    "    ip_addresses = []\n",
    "    for row in rows:\n",
    "        # Get td elements\n",
    "        columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        # Store data in dictionary\n",
    "        data = {\"Proxy address\":\"\", \"Proxy type\":\"\"}\n",
    "        # Check if HTTP or HTTPS\n",
    "        if \"HTTPS\" in columns[1].text.strip():\n",
    "            data[\"Proxy address\"] = columns[0].text.strip()\n",
    "            data[\"Proxy type\"] = \"https\"\n",
    "        else:\n",
    "            data[\"Proxy address\"] = columns[0].text.strip()\n",
    "            data[\"Proxy type\"] = \"http\"\n",
    "        # Append data to list\n",
    "        ip_addresses.append(data)\n",
    "        \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086cc4f",
   "metadata": {},
   "source": [
    "Next, we need to quit the `driver` and save the list of proxies to a Pandas DataFrame. Finally, we'll output the DataFrame to a CSV file, so that our list of proxies will be available for us to use in other scraping programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9087a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proxies():\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    # Save to DataFrame\n",
    "    df = pd.DataFrame(ip_addresses)\n",
    "    # Output to CSV\n",
    "    df.to_csv(\"spys-proxy-list-500.csv\")\n",
    "    \n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217c101",
   "metadata": {},
   "source": [
    "Here is the full `scrape_proxies()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508f4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proxies():\n",
    "    url = \"https://spys.one/en/anonymous-proxy-list/\"\n",
    "\n",
    "    # Instantiate the driver\n",
    "    driver = browser()\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Select option from dropdown menu for \"Show\"\n",
    "    select_show = Select(driver.find_element(By.ID, \"xpp\"))\n",
    "    select_show.select_by_visible_text(\"500\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Select option from dropdown menu for \"Type\"\n",
    "    select_type = Select(driver.find_element(By.ID, \"xf5\"))\n",
    "    select_type.select_by_visible_text(\"HTTP\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Get rows\n",
    "    rows = driver.find_elements(By.TAG_NAME, \"tr\")\n",
    "    # Filter out unnecessary rows\n",
    "    rows = rows[9:len(rows)-4:2]\n",
    "\n",
    "    # Loop through rows\n",
    "    ip_addresses = []\n",
    "    for row in rows:\n",
    "        # Get td elements\n",
    "        columns = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        # Store data in dictionary\n",
    "        data = {\"Proxy address\":\"\", \"Proxy type\":\"\"}\n",
    "        # Check if HTTP or HTTPS\n",
    "        if \"HTTPS\" in columns[1].text.strip():\n",
    "            data[\"Proxy address\"] = columns[0].text.strip()\n",
    "            data[\"Proxy type\"] = \"https\"\n",
    "        else:\n",
    "            data[\"Proxy address\"] = columns[0].text.strip()\n",
    "            data[\"Proxy type\"] = \"http\"\n",
    "        # Append data to list\n",
    "        ip_addresses.append(data)\n",
    "        \n",
    "    driver.quit()\n",
    "\n",
    "    # Save to DataFrame\n",
    "    df = pd.DataFrame(ip_addresses)\n",
    "    # Output to CSV\n",
    "    df.to_csv(\"spys-proxy-list-500.csv\", index=False)\n",
    "    print(\"Scraping completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de123fcb",
   "metadata": {},
   "source": [
    "Now we can run our `scrape_proxies()` function to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb1bec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed\n"
     ]
    }
   ],
   "source": [
    "scrape_proxies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be0638",
   "metadata": {},
   "source": [
    "Sure enough, our output file was created. Let's load it into a DataFrame and print the first five rows to see if the data was scraped correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6b4318a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proxy address</th>\n",
       "      <th>Proxy type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.190.147.158:54018</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203.110.240.166:80</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.20.217.52:8080</td>\n",
       "      <td>https</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144.91.88.75:5566</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.183.143.134:80</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Proxy address Proxy type\n",
       "0  41.190.147.158:54018       http\n",
       "1    203.110.240.166:80       http\n",
       "2      1.20.217.52:8080      https\n",
       "3     144.91.88.75:5566       http\n",
       "4     68.183.143.134:80       http"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spys-proxy-list-500.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d201b",
   "metadata": {},
   "source": [
    "Let's print the last five rows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0711788b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proxy address</th>\n",
       "      <th>Proxy type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>203.154.71.139:80</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>206.81.0.107:80</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>66.196.238.178:3128</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>113.53.94.12:9812</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>41.60.232.131:34098</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Proxy address Proxy type\n",
       "495    203.154.71.139:80       http\n",
       "496      206.81.0.107:80       http\n",
       "497  66.196.238.178:3128       http\n",
       "498    113.53.94.12:9812       http\n",
       "499  41.60.232.131:34098       http"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec93bc",
   "metadata": {},
   "source": [
    "We can see that all 500 rows of data were successfully extracted and saved to the CSV file. Let's write a quick function to randomly select one of these proxies to use for scraping using the `randint` function. Then, we'll format this as the required dictionary which can be used with the Requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81f27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http': 'http://72.47.152.224:55443', 'https': 'https://72.47.152.224:55443'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_random_proxy():\n",
    "    # Load the list of proxies into DataFrame\n",
    "    df = pd.read_csv(\"spys-proxy-list-500.csv\")\n",
    "    # Select random index\n",
    "    proxy_index = random.randint(0, len(df) - 1)\n",
    "    selected_proxy = df.loc[proxy_index, \"Proxy address\"]\n",
    "    # Format proxy\n",
    "    proxy = {\"http\": \"http://\" + selected_proxy, \"https\": \"https://\" + selected_proxy}\n",
    "    return proxy\n",
    "\n",
    "print(get_random_proxy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce892f56",
   "metadata": {},
   "source": [
    "Let's write another function to quickly test our randomly selected proxy. We can submit a GET request to the website [icanhazip](http://icanhazip.com) for this purpose. Printing the content of the resulting response should display the proxy IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e7052b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http': 'http://200.105.215.18:33630', 'https': 'https://200.105.215.18:33630'}\n",
      "b'200.105.215.18\\n'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def test_proxy():\n",
    "    # Get random user agent\n",
    "    ua = UserAgent()\n",
    "    user_agent = ua.random\n",
    "    headers = {\"User-Agent\":user_agent}\n",
    "    # Get random proxy\n",
    "    proxy = get_random_proxy()\n",
    "    print(proxy)\n",
    "    # Test proxy\n",
    "    r = requests.get(\"http://icanhazip.com\", headers=headers, proxies=proxy)\n",
    "    return r.content\n",
    "\n",
    "print(test_proxy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293beda",
   "metadata": {},
   "source": [
    "The randomly selected proxy address matches the output of the call to the `requests.get()` function, so our test was successful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
